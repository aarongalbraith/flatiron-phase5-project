{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00364a94",
   "metadata": {},
   "source": [
    "# Flatiron Phase 5 Project\n",
    "\n",
    "## Aaron Galbraith\n",
    "\n",
    "https://www.linkedin.com/in/aarongalbraith \\\n",
    "https://github.com/aarongalbraith\n",
    "\n",
    "### Submitted: November 20, 2023\n",
    "\n",
    "## Contents\n",
    "\n",
    "- **[Business Understanding](#Business-Understanding)<br>**\n",
    "- **[Data Understanding](#Data-Understanding)**<br>\n",
    "- **[Data Preparation](#Data-Preparation)**<br>\n",
    "- **[Exploration](#Exploration)**<br>\n",
    "- **[Modeling](#Modeling)**<br>\n",
    "- **[Evaluation](#Evaluation)**<br>\n",
    "- **[Recommendations](#Recommendations)<br>**\n",
    "- **[Further Inquiry](#Further-Inquiry)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c51b30",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "In 2022 the US Supreme Court ruled in [Dobbs v. Jackson Women's Health Organization](https://www.supremecourt.gov/opinions/21pdf/19-1392_6j37.pdf) that the United States Constitution would no longer confer a right to abortion, leaving the legality of abortion timelines and procedures to the discretion of the individual states and territories. This brought an end to constitutionally enshrined abortion access after Americans had been guaranteed such rights in all American states and territories for roughly two generations, since the 1973 ruling in [Roe v. Wade](https://tile.loc.gov/storage-services/service/ll/usrep/usrep410/usrep410113/usrep410113.pdf). Both immediately and in the short time since the Dobbs decision, [many states have reduced access to abortion, and 14 states have banned abortion entirely](https://reproductiverights.org/maps/abortion-laws-by-state/). [Advocates for birth control access fear that new government controls could move beyond abortion and attempt to restrict birth control access as well](https://www.npr.org/2022/08/16/1117615628/abortion-birth-control-title-x-supreme-court-family-planning-planned-parenthood).\n",
    "\n",
    "In the new reproductive environment created by this ruling, Americans who are concerned with family planning need to be better informed of their changing options. Relevant conversations take place at two different levels: 1) People candidly share their experiences and concerns with each other, often sharing and reading posts online in online forums such as [Reddit](https://www.reddit.com/r/birthcontrol/) and [Quora](https://www.quora.com/search?q=birth%20control), and 2) people communicate with informed reproductive health care professionals such as [Planned Parenthood](), either through [outreach programs](https://www.plannedparenthood.org/planned-parenthood-greater-new-york/learn/community-programs), reading their [blog](https://www.plannedparenthood.org/blog), or receiving care directly. These two levels of conversation should inform each other. Patients can learn from their professionals and then share what they've learned about various methods' efficacy and side effects with others in online communities, but professionals can also learn about prevalent preferences, concerns, and misconceptions by observing the candid conversations that people have with each other online.\n",
    "\n",
    "In 2018, researchers Surya Kallumadi and Felix Gräßer at UC Irvine created the [UCI ML Drug Review dataset](https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018/) after collecting reviews from [Drugs.com](https://www.drugs.com/) that users had written about various drugs between 2008 and 2017. A substantial portion of these reviews addressed birth control and emergency contraception drugs.\n",
    "\n",
    "Our project seeks to inform reproductive health care professionals about online conversations. We use this collection of birth control treatment reviews to predict 1) the birth control method discussed and 2) the sentiment conveyed in each review. With this tool, Planned Parenthood will have a new way of anticipating their patients' needs and will be better prepared to help them achieve their family planning goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24e4b4",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c20cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/stubbletrouble/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/stubbletrouble/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /Users/stubbletrouble/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\r\n",
      "Requirement already satisfied: pyahocorasick in /Users/stubbletrouble/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import html\n",
    "\n",
    "! pip install contractions\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 1979\n",
    "\n",
    "do_grids = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313869d",
   "metadata": {},
   "source": [
    "## Load and briefly explore data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5934c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://raw.githubusercontent.com/aarongalbraith/flatiron-phase5-project/main/data/drugsComTrain_raw.tsv'\n",
    "url2 = 'https://raw.githubusercontent.com/aarongalbraith/flatiron-phase5-project/main/data/drugsComTest_raw.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb31c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(url1, delimiter='\\t', encoding='latin-1')\n",
    "d2 = pd.read_csv(url2, delimiter='\\t', encoding='latin-1')\n",
    "df = pd.concat([d1,d2]).reset_index().drop(columns=['Unnamed: 0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b0dd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   drugName                     condition  \\\n",
       "0                 Valsartan  Left Ventricular Dysfunction   \n",
       "1                Guanfacine                          ADHD   \n",
       "2                    Lybrel                 Birth Control   \n",
       "3                Ortho Evra                 Birth Control   \n",
       "4  Buprenorphine / naloxone             Opiate Dependence   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "3  \"This is my first time using any form of birth...     8.0   \n",
       "4  \"Suboxone has completely turned my life around...     9.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  \n",
       "3   November 3, 2015           10  \n",
       "4  November 27, 2016           37  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6a45b",
   "metadata": {},
   "source": [
    "There are some missing `condition` labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f454a3",
   "metadata": {},
   "source": [
    "### `drugName` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b33b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drugName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48129d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drugName.value_counts().quantile(.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a6cff",
   "metadata": {},
   "source": [
    "There are 3,671 unique drug names, and 10% of the drug names have more than 120 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e563ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(df.drugName.value_counts())\n",
    "pd.set_option(\"display.max_rows\", 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee070c",
   "metadata": {},
   "source": [
    "A casual overview of the drug names indicates that they all seem valid. Some seem to specify drug combinations and/or dosage amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7f652",
   "metadata": {},
   "source": [
    "### `condition` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts().quantile(.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd5643",
   "metadata": {},
   "source": [
    "There are 916 unique conditions, and 10% of the conditions have more than 332 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08713529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(df.condition.value_counts())\n",
    "pd.set_option(\"display.max_rows\", 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b735f",
   "metadata": {},
   "source": [
    "Oddly, the condition labels often (always?) omit initial 'F' and terminal 'r'. We can isolate instances of the former by searching for conditions that start with a lower case letter.\n",
    "\n",
    "We will eventually trim our records to a number of conditions that Planned Parenthood specializes in treating (and perhaps birth control exclusively), but we will need all the records to help us determine missing condition labels. After we have restored (or discarded) all missing condition labels, we can drop the conditions outside the scope of this review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca501f",
   "metadata": {},
   "source": [
    "### `drugName` × `condition` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('drugName').condition.nunique().value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2fe3ce",
   "metadata": {},
   "source": [
    "This means that, for example, 1869 drugs treat 1 condition only, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('condition').drugName.nunique().value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ac2cf",
   "metadata": {},
   "source": [
    "This means that 188 conditions are treatable by two drugs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d2698",
   "metadata": {},
   "source": [
    "### `review` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefa11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722abe4d",
   "metadata": {},
   "source": [
    "This suggests that just over half of the review values are unique. Almost certainly there will be some duplication issues to deal with.\n",
    "\n",
    "Let's look at several reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f644498",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(df.review[i], '\\n-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3cd97c",
   "metadata": {},
   "source": [
    "There appear to be escaped characters (e.g. `&#039;`, indicating an apostrophe) and contractions. We can address this now without affecting our analysis.\n",
    "\n",
    "We'll reset the review texts to unescape these characters and expand all contractions.\n",
    "\n",
    "Note: This will replace all instances of `ain't` with `are not`, resulting in some subject-verb agreement issues (e.g. `I are not`). This difference will be negligible in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c37037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(lambda x: contractions.fix(html.unescape(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21c785",
   "metadata": {},
   "source": [
    "### `rating` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a99cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.hist(bins=df.rating.nunique());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27dc3f",
   "metadata": {},
   "source": [
    "Most of the conditions lie at the extremes, and more of them appear to be at the positive extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c99c06",
   "metadata": {},
   "source": [
    "### `date` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206cb05",
   "metadata": {},
   "source": [
    "In order to get a better understanding of the `date` feature, we'll convert it to a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = df.date.apply(lambda x: datetime.strptime(x, '%B %d, %Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.datetime.hist(bins=80);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df.datetime.min().strftime('%B %d, %Y')\n",
    "end_date = df.datetime.max().strftime('%B %d, %Y')\n",
    "\n",
    "print('The reviews span specifically from', start_date+',', 'to', end_date+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6afc5",
   "metadata": {},
   "source": [
    "The reviews began to surge in early 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480379c",
   "metadata": {},
   "source": [
    "### `usefulCount` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulCountCumulative = list(df.usefulCount.values)\n",
    "usefulCountCumulative.sort(reverse=True)\n",
    "\n",
    "X = range(0,len(df),5000)\n",
    "Y = []\n",
    "for x in X:\n",
    "    Y.append(sum(usefulCountCumulative[:x]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X,Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713c01a",
   "metadata": {},
   "source": [
    "(If all reviews were equally useful, this graph would be a straight diagonal line from SW to NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.usefulCount.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faaf4c",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c815ab",
   "metadata": {},
   "source": [
    "## An outline for getting the information we need\n",
    "\n",
    "In addition to the standard goals of data cleaning, the main goals of our data preparation are\n",
    "1. Drop any records that don't pertain to birth control\n",
    "2. Identify which method of birth control each review discusses\n",
    "\n",
    "The records indicate a drug name and what condition (e.g. birth control) it treats, but problems with both of these features abound. There are many missing condition labels, and we can only restore them (and find out which ones should also have the \"birth control\" label) by understanding how they associate with the drug name labels. Some drug labels are for brand names, and others are for the generic or chemical name of the drug. Many records are actually instances of the review being entered twice, once with the brand name and once with the generic name. That's not the end of the complications, but it will suffice for an overview.\n",
    "\n",
    "Once the first main goal is achieved and we are able to trim the records to only those that concern the condition of birth control, we can use the edited and matched brand/generic names to identify the birth control method each drug is an instance of (e.g. injectable, IUD, patch). This also involved, for example, consolidating certain versions of the same brand name together to reduce the number of brands and make for more tractable analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d5445",
   "metadata": {},
   "source": [
    "## Missing and erroneous condition labels\n",
    "\n",
    "In this section we will identify all `condition` labels that are either missing or in need of editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2364ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d576d8",
   "metadata": {},
   "source": [
    "We noticed another `condition` label that was meant to indicate missing and should be accordingly changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition = df.condition.apply(lambda x: 'missing' if 'Not Listed' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442982f",
   "metadata": {},
   "source": [
    "We've identified some actual missing `condition` labels, but we noticed there are more `condition` labels that seem suspicious, particularly ones that start with something other than an upper case character. Let's look at all such `condition` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da673ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[(~df.condition.str[0].isin(list(string.ascii_uppercase))) &\n",
    "   (df.condition != 'missing')\n",
    "  ].condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae2123",
   "metadata": {},
   "source": [
    "These fall into three categories:\n",
    "1. \"X users found this comment helpful\" should be regarded as an erroneous label and retagged as \"missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4aa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition = df.condition.apply(lambda x: 'missing' if 'users found' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf658e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77309c0d",
   "metadata": {},
   "source": [
    "2. Labels that show a clipped copy of the `drugName` label and end with a parenthesis should also be regarded as missing. These erroneous labels merely repeat information already available in the `drugName` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dedea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition = df.condition.apply(lambda x: 'missing' \\\n",
    "                                  if x[0] not in list(string.ascii_uppercase) and \\\n",
    "                                  x[-1] in ['(', ')'] \\\n",
    "                                  else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64829675",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb04f1e",
   "metadata": {},
   "source": [
    "3. Other `condition` labels appear to omit the first and/or last several characters. We can infer certain corrections here to restore many of the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e51057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_restore(condition):\n",
    "    if condition.split()[-1] in ['Disorde', 'eve', 'Shoulde', 'Cance']:\n",
    "        condition = condition+'r'\n",
    "    if condition.split()[0] in ['acial', 'ibrocystic', 'ungal', 'amilial', 'ailure', 'ever', \\\n",
    "                                'emale', 'unctional', 'actor', 'ibromyalgia', 'atigue']:\n",
    "        condition = 'F'+condition\n",
    "    if condition.split()[0] in ['llicular', 'llicle', 'lic', 'cal']:\n",
    "        condition = 'Fo'+condition\n",
    "    if condition.split()[0] in ['mance']:\n",
    "        condition = 'Perfor'+condition\n",
    "    if condition.split()[0] in ['zen']:\n",
    "        condition = 'Fro'+condition\n",
    "    if condition.split()[0] in ['mis']:\n",
    "        condition = 'Dermatitis Herpetifor'+condition\n",
    "    return condition\n",
    "\n",
    "df.condition = df.condition.apply(lambda x: condition_restore(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23fa614",
   "metadata": {},
   "source": [
    "Let's look at what we have left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[(~df.condition.str[0].isin(list(string.ascii_uppercase))) &\n",
    "   (df.condition != 'missing')\n",
    "  ].condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c705f9",
   "metadata": {},
   "source": [
    "\"von Willebrand's Disease\" appears to be a naturally uncapitalized condition. The others have been impossible to restore and will also be regarded as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition = df.condition.apply(lambda x: 'missing' \\\n",
    "                                  if x[0] not in list(string.ascii_uppercase) and \\\n",
    "                                  x.split()[0] != 'von' \\\n",
    "                                  else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8350496",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc5470",
   "metadata": {},
   "source": [
    "We will be able to restore more of these missing condition labels after we do some work with duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0452f",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964809a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_duplicates = True\n",
    "\n",
    "def show_review(index):\n",
    "    if show_duplicates:\n",
    "        display(df[df.review == df.loc[index].review][['drugName', 'condition', 'rating', 'date', 'usefulCount']])\n",
    "    print('\\nReview #'+str(index),'| Rating:',df.loc[index].rating,'| Upvotes:',\n",
    "          df.loc[index].usefulCount,'\\n\\n'+df.review.loc[index][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_review(178703)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c5735",
   "metadata": {},
   "source": [
    "For reasons we will explore later, we believe this review was submitted twice by the same person, that each instance of it happened to receive 10 upvotes, and that it should correctly be associated with a grand total of 20 upvotes. Because this is one special instance where the review happened to receive 10 upvotes both times, making it a true duplicate of the data set, we will fix the values here, lest it interfere with operations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8057bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[178703, 'usefulCount'] = 20\n",
    "df.at[191001, 'usefulCount'] = 20\n",
    "df.drop([131531, 143768], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf969c",
   "metadata": {},
   "source": [
    "## Duplicates due to brand / generic pairs\n",
    "\n",
    "The main type of duplicate we should look out for is records with duplicate reviews, as those likely indicate some kind of actual erroneous duplication. Let's see how many of those there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['review']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f6a1e",
   "metadata": {},
   "source": [
    "That's a lot!\n",
    "\n",
    "Let's explore some facets of these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff025c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.duplicated(subset=df.columns.difference(['drugName']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b265c2",
   "metadata": {},
   "source": [
    "The vast majority of duplicate reviews are accounted for by different drug names. Let's explore some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[df.duplicated(subset=df.columns.difference(['drugName']))].index[:5]:\n",
    "    show_review(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5b7f3",
   "metadata": {},
   "source": [
    "These five examples make clear that the vast majority of duplicates are due to double-entry; (nearly) every review is entered once with its generic name and once with its brand name.\n",
    "\n",
    "We can use this phenomenon to restore some of the missing condition labels. If a missing condition label is part of such a unique pair, then we can confidently assign it the condition of its pair-mate.\n",
    "\n",
    "Let's broaden our search to records that duplicate every feature other than drug name and condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.duplicated(subset=df.columns.difference(['drugName', 'condition']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06562c2",
   "metadata": {},
   "source": [
    "This is how many records are duplicates of other records in all values EXCEPT (POSSIBLY) drug name and condition. If a record is duplicated in this manner, the second (and third, fourth, etc.) instance will be captured in this bucket of dupes.\n",
    "\n",
    "If we check only this bucket for dupes, we can see whether there are any triplets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d190a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dupes = df[df.duplicated(subset=df.columns.difference(['drugName', 'condition']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dupes[df_dupes.duplicated(subset=df_dupes.columns.difference(['drugName', 'condition']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8076a67",
   "metadata": {},
   "source": [
    "There is only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dupes[df_dupes.duplicated(subset=df_dupes.columns.difference(['drugName', 'condition']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6895c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_review(140144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734bf74",
   "metadata": {},
   "source": [
    "There are 6 records with the same review, date, rating, and condition. (The reviews on October 5, 2012, appear to be just a coincidence of the same review wording for a different drug and condition.) Because they're on the *same day*, it seems likely that these reviews were possibly entered repeatedly by the same person. The two with a useful count of 10 are likely a brand/generic pair.\n",
    "\n",
    "As for the other 4, it's not clear what is going on. We will (would) later discover that there is also some discrepancy as to which of these is a brand or generic name. Since the review text isn't very descriptive, and the useful count is so low, (and it doesn't pertain to the main conditions treated by Planned Parenthood), let's just drop all 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6611747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([60998, 119972, 133212, 140144], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d17de1",
   "metadata": {},
   "source": [
    "Now we should be able to create a list of pairs of indices of records that match in all features except possibly drug name and condition. To make this run faster, we'll first create a way to sort them by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ⏰ record the time for this cell -- usually 11-12 s\n",
    "\n",
    "# create stripped down dataframe that does not have drug names or conditions\n",
    "# we don't need these features for this operation because we're checking for matches on all other features\n",
    "df_pairs = df.drop(columns=['drugName', 'condition']).copy()\n",
    "\n",
    "# create a list of indices of records that duplicate everything other than drug name and condition\n",
    "df_dupes = df_pairs[df_pairs.duplicated()].index.tolist().copy()\n",
    "\n",
    "# create and populate a dictionary whose keys are dates and whose values are indices\n",
    "dates_dict = {}\n",
    "# populate dictionary with keys that are dates belonging to the duplicates\n",
    "for date_ in list(set(df[df.index.isin(df_dupes)].date.tolist())):\n",
    "    dates_dict[date_] = []\n",
    "# populate dictionary with values that are indices that are NOT from the duplicate list but DO share that date\n",
    "for i in df[~df.index.isin(df_dupes)].index:\n",
    "    dates_dict[df.loc[i].date].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c22bb8",
   "metadata": {},
   "source": [
    "Now we can use this dates dictionary to sort and identify the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ⏰ record the time for this cell -- usually 2–4 mins\n",
    "\n",
    "# create a list of record pairs where each entry is a list of two indices\n",
    "pairs = []\n",
    "\n",
    "# iterate over the indices from the dupes list\n",
    "for i in df_dupes:\n",
    "    # set the date to the date from index i\n",
    "    date_i = df.loc[i].date\n",
    "    # iterate over OTHER indices who share that date\n",
    "    for j in dates_dict[date_i]:\n",
    "        # check for a match\n",
    "        if df_pairs.loc[i].equals(df_pairs.loc[j]) and df.drugName.loc[i] != df.drugName.loc[j]:\n",
    "            # remove this index from the dates dictionary so we have fewer to search through in later iterations\n",
    "            dates_dict[date_i].remove(j)\n",
    "            # add this pair to the pairs list\n",
    "            pairs.append([i,j])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249abdca",
   "metadata": {},
   "source": [
    "Let's take a look at several of the pairs we've collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae49ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2480a81",
   "metadata": {},
   "source": [
    "Here we'll create a dictionary that matches the index of one pair member to the other member of the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_dict = {}\n",
    "\n",
    "for pair in pairs:\n",
    "    for i in range(2):\n",
    "        pairs_dict[pair[i]] = pair[1-i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabc039",
   "metadata": {},
   "source": [
    "## Restore missing `condition` labels\n",
    "\n",
    "We will restore missing `condition` labels in two ways, in order of certainty:\n",
    "\n",
    "1. For missing values that possess a pair match, we will assign it the condition of its match.\n",
    "2. For the remaining missing values, we will assign it the condition that is most commonly associated with its drug name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ⏰ record the time for this cell -- usually 10-15 seconds\n",
    "\n",
    "# iterate over each record pair\n",
    "for pair in pairs:\n",
    "    # iterate over each member of the pair\n",
    "    for i in range(2):\n",
    "        # identify a pair member whose condition is missing\n",
    "        if df.loc[pair[i]].condition == 'missing':\n",
    "            # assign to the pair member the condition of its pair-mate\n",
    "            df.at[pair[i], 'condition'] = df.loc[pairs_dict[pair[i]]].condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a411da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d963608",
   "metadata": {},
   "source": [
    "Because it will be useful later, we'll make a feature that names the indicated drug and, if applicable, the paired drug.\n",
    "\n",
    "This is not a *final* replacement for the drug name feature, but it will allow us to better recognize the relationship between the generic and brand drug names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e04074",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ⏰ record the time for this cell -- usually 15-30 seconds\n",
    "\n",
    "df['ind'] = df.index\n",
    "\n",
    "def drugList_fix(index, drugName_):\n",
    "    drugList = [drugName_]\n",
    "    if index in pairs_dict:\n",
    "        drugList.append(df.loc[pairs_dict[index]].drugName)\n",
    "        # alphabetize each drug pair so that we will not mistakenly duplicate e.g. [A,B] & [B,A]\n",
    "        drugList.sort()\n",
    "    return drugList\n",
    "\n",
    "df['drugList'] = df.apply(lambda x: drugList_fix(x.ind, x.drugName), axis=1)\n",
    "\n",
    "df.drop(columns='ind', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8e684",
   "metadata": {},
   "source": [
    "Now we can create a feature that tells us if a record is associated with a paired drug name or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e47402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isPaired'] = df.drugList.apply(lambda x: True if len(x) > 1 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.isPaired])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[~df.isPaired])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ee7cf",
   "metadata": {},
   "source": [
    "Because lists confuse certain operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['drugSetString'] = df.drugList.apply(lambda x: x[0] + ' ' + x[1] if len(x) == 2 else x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3578a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df[df.duplicated(subset=df.columns.difference(['drugName', 'drugSet', 'drugList']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee1fd1",
   "metadata": {},
   "source": [
    "With this new feature in place, we can drop one record from each of the brand/generic pairs. The drug name feature will retain only one member of the pair -- unpredictably either the brand or the generic -- which will make this feature more or less useless for the moment.\n",
    "\n",
    "Before we drop these records, we'll create a bookmark copy of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ca506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT re-run this cell out of sequence\n",
    "# to use the dataframe as it was at this stage, un-comment, run, and re-comment the cell that follows after it.\n",
    "df_bookmark_1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_bookmark_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbc2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=df.columns.difference(['drugName', 'drugSet', 'drugList']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4d222",
   "metadata": {},
   "source": [
    "Explain why we're doing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_duplicates = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017ea0b",
   "metadata": {},
   "source": [
    "For every remaining record with a missing condition, we will assign it the condition that is most common for the drug indicated by that record. (This will not be biased by duplicates from brand/generic pairs, because we have dropped those duplicates.)\n",
    "\n",
    "This will be the last use we have for conditions *not* treated by Planned Parenthood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_w_missing_condition = list(set(df[df.condition == 'missing'].drugSetString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drugs_w_missing_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drugSetString.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387869b2",
   "metadata": {},
   "source": [
    "This applies to some 20% of the drugs. We'll create a dictionary that reports the most common condition for these drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03687090",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# record the time for this cell -- 10-20 seconds\n",
    "\n",
    "most_common_condition = {}\n",
    "\n",
    "for drug in drugs_w_missing_condition:\n",
    "    condition = df[df.drugSetString == drug].condition.value_counts().idxmax()\n",
    "    if condition == 'missing' and len(set(df[df.drugSetString == drug].condition)) > 1:\n",
    "        condition = df[(df.drugSetString == drug) &\n",
    "                       (df.condition != 'missing')\n",
    "                      ].condition.value_counts().idxmax()\n",
    "    proportion = round(df[df.drugSetString == drug].condition.value_counts(normalize=True)[0],2)\n",
    "    most_common_condition[drug] = [condition, proportion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_condition['Sildenafil Viagra']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec94c2a",
   "metadata": {},
   "source": [
    "For example, if a review with an unlisted condition is about Viagra, we will assume the condition is Erectile \n",
    "Dysfunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'] = df.apply(lambda x: most_common_condition[x.drugSetString][0] \\\n",
    "                           if x.condition == 'missing' \\\n",
    "                           else x.condition, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3d5b2",
   "metadata": {},
   "source": [
    "This is how many records there are that still have no condition label. This means the drugs indicated in these records are *only* indicated in references without an indicated condition. As such, there's not really anything we can do with these records, and we may as well drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37196468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.condition == 'missing'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80789e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.condition == 'missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94d4ea",
   "metadata": {},
   "source": [
    "## Drop records by condition\n",
    "\n",
    "At this point, we still have more cleaning to do, but we have identified all the conditions that we can, and we won't have any further need for records with certain condition values, so we'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT re-run this cell out of sequence\n",
    "# to use the dataframe as it was at this stage, un-comment, run, and re-comment the cell that follows after it.\n",
    "df_bookmark_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_bookmark_2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b97e6",
   "metadata": {},
   "source": [
    "Let's take another look at the complete list of conditions and choose which ones to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc69429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e69fb",
   "metadata": {},
   "source": [
    "Since there are so many conditions to consider, let's limit this to just conditions with at least 25 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd601c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "display(df['condition'].value_counts().loc[lambda x: x >= 25])\n",
    "pd.set_option(\"display.max_rows\", 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[~df.condition.isin(['Birth Control', 'Emergency Contraception'])].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02514d26",
   "metadata": {},
   "source": [
    "## Pairing generic and brand names\n",
    "\n",
    "Now that we have a smaller number of records to deal with, we can sort out generic and brand names.\n",
    "\n",
    "First we'll create a list of all values from the drug name feature. (Some of these have been dropped from the drug name feature itself when we dropped one record from each brand/generic pair, but all of them were included in the drug list feature.)\n",
    "\n",
    "We'll create two lists: paired drugs (which we will attempt to sort into brand and generic) and single drugs (each of which we will then try to identify as either brand or generic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drug_lists = df.drugList.tolist()\n",
    "all_drug_lists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34323b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_drug_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ab7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drug_names = set()\n",
    "\n",
    "for list_ in all_drug_lists:\n",
    "    all_drug_names.add(list_[0])\n",
    "    if len(list_) > 1:\n",
    "        all_drug_names.add(list_[1])\n",
    "\n",
    "all_drug_names = list(all_drug_names)\n",
    "\n",
    "all_drug_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_drug_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84914a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will create a full list with duplicates\n",
    "# we need to do this intermediate before moving to the following step to remove duplicates\n",
    "paired_drug_lists = df[df.isPaired].drugList.tolist()\n",
    "paired_drug_lists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paired_drug_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_drug_names = set()\n",
    "\n",
    "for pair in paired_drug_lists:\n",
    "    paired_drug_names.add(pair[0])\n",
    "    paired_drug_names.add(pair[1])\n",
    "\n",
    "paired_drug_names = list(paired_drug_names)\n",
    "\n",
    "paired_drug_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpaired_drug_names = [drug for drug in all_drug_names if drug not in paired_drug_names]\n",
    "\n",
    "unpaired_drug_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paired_drug_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unpaired_drug_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec89a1",
   "metadata": {},
   "source": [
    "Together, these two lists of names constitute all the drug names left to sort into brand and generic categories.\n",
    "\n",
    "In order to sort the list of paired drugs into brand and generic, we'll establish a dictionary whose keys are all the drug names that appear in a generic/brand pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa485d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dict = {}\n",
    "\n",
    "for drug in paired_drug_names:\n",
    "    drug_dict[drug] = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a174b",
   "metadata": {},
   "source": [
    "We'll assign values to those keys according to the pairings. For example, if drug name A is in a generic/brand pair with drug name B, then they will appear on each other's list of values in this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in paired_drug_lists:\n",
    "    drug_dict[pair[0]].add(pair[1])\n",
    "    drug_dict[pair[1]].add(pair[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff56019",
   "metadata": {},
   "source": [
    "Let's find out how many of these drug names are associated with exactly one other drug name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len({drug for drug in drug_dict if len(drug_dict[drug]) == 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c23021",
   "metadata": {},
   "source": [
    "That should mean that exactly the remainder are associated with multiple drug names. It would make sense that drug names that belong to multiple generic/brand pairs are themselves the generic name. On that assumption, we'll create a list of generic drug names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ba442",
   "metadata": {},
   "outputs": [],
   "source": [
    "generics = [drug for drug in drug_dict if len(drug_dict[drug]) > 1]\n",
    "\n",
    "generics.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833120d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822da29",
   "metadata": {},
   "source": [
    "Now we'll check to make sure that the drug names we've just designated as \"generic\" do NOT belong to a generic/brand pair with *another* \"generic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for drug in generics:\n",
    "    for match in drug_dict[drug]:\n",
    "        if match in generics:\n",
    "            count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7983b",
   "metadata": {},
   "source": [
    "Great.\n",
    "\n",
    "Then we can begin designating drug names as \"brands\" if they are in a generic/brand pair with a generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = set()\n",
    "\n",
    "for generic in generics:\n",
    "    for match in drug_dict[generic]:\n",
    "        brands.add(match)\n",
    "\n",
    "brands = list(brands)\n",
    "\n",
    "brands.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e79ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df75c16",
   "metadata": {},
   "source": [
    "Now let's see what drugs remain and how many records they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorized = list(set(drug for drug in paired_drug_names if drug not in generics and drug not in brands))\n",
    "\n",
    "uncategorized.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040eafd",
   "metadata": {},
   "source": [
    "To be clear, these are drug names with the following properties:\n",
    "\n",
    "- the drug name belongs to an exclusive brand/generic pair\n",
    "- we have not yet identified which members of the pair are brand and generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uncategorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf366eb",
   "metadata": {},
   "source": [
    "We should be able to list all of these drug names in their pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02064bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated = set()\n",
    "for drug in uncategorized:\n",
    "    if drug not in repeated:\n",
    "        print(drug, '||', list(drug_dict[drug])[0])\n",
    "        repeated.add(drug)\n",
    "        repeated.add(list(drug_dict[drug])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35986c",
   "metadata": {},
   "source": [
    "With so few pairs, we can Google the names to determine which names of a pair are generic and brand names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43238ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_brands = [\n",
    "    'ParaGard', 'Natazia', 'NuvaRing', 'Necon 1 / 50', 'ella'\n",
    "]\n",
    "\n",
    "brands.extend(new_brands)\n",
    "\n",
    "len(brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bcc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for drug in new_brands:\n",
    "    generics.append(list(drug_dict[drug])[0])\n",
    "\n",
    "len(generics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082a1ee",
   "metadata": {},
   "source": [
    "At this point, we have sorted all the paired brand and generic drug names. What remains is to identify whether each of the single drug names is a generic or brand name.\n",
    "\n",
    "Let's look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01545a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpaired_drug_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4948762",
   "metadata": {},
   "source": [
    "Simple Google search confirms these are both generic names, so we'll add them as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "generics.extend(unpaired_drug_names)\n",
    "\n",
    "generics.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59935c9",
   "metadata": {},
   "source": [
    "Now we create a more universal drug naming system whereby every record is identified with its generic name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_fix(drugList):\n",
    "    if len(drugList) == 1 or drugList[0] in generics:\n",
    "        return drugList[0]\n",
    "    else:\n",
    "        return drugList[1]\n",
    "\n",
    "df['genericName'] = df.drugList.apply(lambda x: generic_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_brand_fix(drugList):\n",
    "    if len(drugList) == 1:\n",
    "        return None\n",
    "    elif drugList[0] in brands:\n",
    "        return drugList[0]\n",
    "    else:\n",
    "        return drugList[1]\n",
    "\n",
    "df['fullBrandName'] = df.drugList.apply(lambda x: full_brand_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_dict = {}\n",
    "\n",
    "for fullName in brands:\n",
    "    name = fullName\n",
    "    tail = name.split()[-1]\n",
    "    while tail.isnumeric() or tail in ['Fe', 'Lo', 'One-Step', '/', '1.5', 'Contraceptive']:\n",
    "        name = name[:len(name)-len(tail)-1]\n",
    "        tail = name.split()[-1]\n",
    "    head = name.split()[0]\n",
    "    while head in ['Lo', '/']:\n",
    "        name = name[len(head)+1:]\n",
    "        head = name.split()[0]\n",
    "    brand_dict[fullName] = name\n",
    "\n",
    "df['shortBrandName'] = df.fullBrandName.apply(lambda x: None if x == None else brand_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327880e",
   "metadata": {},
   "source": [
    "## Duplicates due to multiple user entry\n",
    "\n",
    "Now we'll turn to more possible duplicate instances. We suspect the same user has copy-pasted an identical review multiple times when that verbatim review appears for the same condition and (generic) drug name with the same rating. Let's look at all such instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.duplicated(subset=['genericName', 'condition', 'review', 'rating'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b88f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=['genericName', 'condition', 'review', 'rating'])] \\\n",
    "[['genericName', 'condition', 'review', 'rating', 'date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6744e0",
   "metadata": {},
   "source": [
    "The review texts all appear to be unique. As long as the review and its duplicate appear close in time to one another (within days), then these should be collapsed into a single review with the respective useful counts added together.\n",
    "\n",
    "First we'll check on the dates. The following cell will show the respective dates of when these duplicated reviews appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[df.duplicated(subset=['genericName', 'condition', 'review', 'rating'])].index:\n",
    "    two_indices = list(df[df.review == df.loc[ind].review].index)\n",
    "    print(df.loc[two_indices[0]].date, '... and ...', df.loc[two_indices[1]].date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855c4dd",
   "metadata": {},
   "source": [
    "They're all identical dates except one that is a day apart.\n",
    "\n",
    "We'll collapse these into single records and add the useful counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bff31b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ind in df[df.duplicated(subset=['genericName', 'condition', 'review', 'rating'])].index:\n",
    "    two_indices = list(df[df.review == df.loc[ind].review].index)\n",
    "    x, y = two_indices[0], two_indices[1]\n",
    "    count = int(df.loc[x].usefulCount + df.loc[y].usefulCount)\n",
    "    df.at[x, 'usefulCount'] = count\n",
    "    df.drop([y], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abecd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT re-run this cell out of sequence\n",
    "# to use the dataframe as it was at this stage, un-comment, run, and re-comment the cell that follows after it.\n",
    "df_bookmark_3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f95f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_bookmark_3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544dab5",
   "metadata": {},
   "source": [
    "## Identifying birth control method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30a275",
   "metadata": {},
   "source": [
    "Consider redoing this, where you pass through everything one time and assign everything to its method (other than pill) and assign everything that remains to pill by default.\n",
    "\n",
    "The problem with this would be the nonoxynol and the levonorgestrel. The former is a spermicide, which we're dropping because there are only two records. The latter we determined applied to emergency (not pill!) ... BUT what if there are brands associated with \"the pill\" that are also levonorgestrel? This might be worth investigating either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_control_dict = {\n",
    "    'IUD': ['Skyla', 'Mirena', 'Kyleena', 'Liletta', 'ParaGard'],\n",
    "    'patch': ['Ortho Evra', 'Xulane'],\n",
    "    'implantable': ['Implanon', 'Nexplanon'],\n",
    "    'vaginal': ['NuvaRing'],\n",
    "    'injectable': ['Depo-Provera', 'depo-subQ provera', 'Provera'],\n",
    "    'emergency': ['Plan B', 'ella', 'Fallback Solo', 'Aftera', 'Take Action', 'Next Choice',\n",
    "                       'My Way', 'EContra EZ'],\n",
    "    'pill': ['Yasmin', 'Ortho Tri-Cyclen', 'Alesse', 'Aviane', 'Sprintec', 'Tri-Sprintec', 'Mircette',\n",
    "             'Seasonique', 'Yaz', 'Lutera', 'Portia', 'Camila', 'Apri', 'Beyaz', 'Desogen', 'Kariva',\n",
    "             'TriNessa', 'Zarah', 'Estarylla', 'Mononessa', 'Gianvi', 'Jolivette', 'Loestrin', 'Microgestin',\n",
    "             'Ortho-Cyclen', 'Ortho-Novum', 'Necon', 'Femcon', 'Marlissa', 'Aubra', 'Viorele', 'Vestura',\n",
    "             'Norlyda', 'Ortho Cyclen', 'Lybrel', 'Pirmella', 'Larin', 'Tarina', 'Previfem', 'Tri-Estarylla',\n",
    "             'Lessina', 'Elinest', 'Cryselle', 'Ortho-Cept', 'Falmina', 'Altavera', 'Tri-Lo-Marzia', 'Taytulla',\n",
    "             'CamreseLo', 'Philith', 'Dasetta', 'Gildess', 'Ovral', 'Jencycla', 'Tri-Linyah', 'Enskyce',\n",
    "             'Orsythia', 'Sronyx', 'Velivet', 'Reclipsen', 'Nikki', 'Levlen', 'Loryna', 'Juleber', 'Trivora',\n",
    "             'Zenchent', 'Tri-Previfem', 'Lyza', 'Seasonale', 'Mono-Linyah', 'Alyacen', 'Opcicon']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_fix_1(shortBrandName):\n",
    "    for method in birth_control_dict:\n",
    "        if shortBrandName in birth_control_dict[method]:\n",
    "            return method\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['method'] = df.shortBrandName.apply(lambda x: None if x == None else method_fix_1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_dict = {\n",
    "    'patch': ['Ethinyl estradiol / norelgestromin'],\n",
    "    'IUD': ['Copper'],\n",
    "    'implantable': ['Etonogestrel'],\n",
    "    'emergency': ['Ulipristal'],\n",
    "    'vaginal': ['Ethinyl estradiol / etonogestrel'],\n",
    "    'pill': ['Ethinyl estradiol / levonorgestrel',\n",
    "             'Drospirenone / ethinyl estradiol / levomefolate calcium',  'Mestranol / norethindrone',\n",
    "             'Ethinyl estradiol / norgestimate', 'Ethinyl estradiol / norethindrone',\n",
    "             'Norethindrone', 'Drospirenone / ethinyl estradiol', 'Desogestrel / ethinyl estradiol',\n",
    "             'Ethinyl estradiol / norgestrel', 'Ethinyl estradiol / folic acid / levonorgestrel',\n",
    "             'Ethinyl estradiol / ethynodiol', 'Dienogest / estradiol'],\n",
    "    'injectable': ['Medroxyprogesterone']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_fix_2(generic):\n",
    "    for method in generic_dict:\n",
    "        if generic in generic_dict[method]:\n",
    "            return method\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['method'] = df.apply(lambda x: method_fix_2(x.genericName) if x.method == None else x.method, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    (df.method.isna())\n",
    "].genericName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0237ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[df.genericName == 'Nonoxynol 9'].index:\n",
    "    show_review(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be067d8",
   "metadata": {},
   "source": [
    "We could create a spermicide label, but there would only be two records for it, so we'll just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([172606, 209857], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601083c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[\n",
    "    (df.genericName == 'Levonorgestrel') &\n",
    "    (df.method.isna())\n",
    "].index[:10]:\n",
    "    show_review(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e55e41",
   "metadata": {},
   "source": [
    "All of these examples describe emergency contraception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_dict['emergency'].append('Levonorgestrel')\n",
    "df['method'] = df.apply(lambda x: method_fix_2(x.genericName) if x.method == None else x.method, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee548f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    (df.method.isna())\n",
    "].genericName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT re-run this cell out of sequence\n",
    "# to use the dataframe as it was at this stage, un-comment, run, and re-comment the cell that follows after it.\n",
    "df_bookmark_4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b424db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_bookmark_4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6c3f0",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264af4c",
   "metadata": {},
   "source": [
    "## Rating over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98336851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rating for each method\n",
    "average_ratings = df.groupby('method')['rating'].mean().reset_index()\n",
    "\n",
    "# Create a bar chart using Seaborn\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Set the font scale to increase text size\n",
    "sns.set(font_scale=1.5)\n",
    "sns.barplot(\n",
    "    x='method',\n",
    "    y='rating',\n",
    "    data=average_ratings,\n",
    "    order=average_ratings.sort_values('rating', ascending=False)['method'],\n",
    "    palette='deep'\n",
    ")\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating for Each Birth Control Method')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from datetime\n",
    "df['year'] = df['datetime'].dt.year\n",
    "\n",
    "# Group by 'year' and 'method', calculate the average rating for each group\n",
    "average_ratings = df.groupby(['year', 'method'])['rating'].mean().reset_index()\n",
    "\n",
    "# Pivot the DataFrame to have 'method' as columns and 'year' as index\n",
    "pivot_table = average_ratings.pivot_table(index='year', columns='method', values='rating')\n",
    "\n",
    "# Plot the data\n",
    "pivot_table.plot(kind='line', marker='o', figsize=(14, 6))\n",
    "plt.title('Average Rating of Each Method by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.legend(title='Method', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa877afe",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Ratings generally declined after the number of ratings increased significantly in 2015.\n",
    "- Emergency contraception has largely been more popular than other methods.\n",
    "- IUDs, patches, and vaginal contraceptives tend to be more popular than pills and implantables.\n",
    "- Injectable methods are consistently the least popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af47a26",
   "metadata": {},
   "source": [
    "## Rating and upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each rating\n",
    "rating_counts = df['rating'].value_counts()\n",
    "\n",
    "# Sort the index for better visualization\n",
    "rating_counts = rating_counts.sort_index()\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(rating_counts.index, rating_counts.values, color='skyblue')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e513fb3",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- There tend to be more positive ratings.\n",
    "- Negative ratings are concentrated on the worst rating (1.0).\n",
    "- There is a slight peak at 5.0. This indicates an attempt at neutrality.\n",
    "\n",
    "In light of this distribution, we could recommend a 3-, 4-, or 5-point rating system that might be a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average usefulCount for each rating\n",
    "average_useful_count = df.groupby('rating')['usefulCount'].mean().reset_index()\n",
    "\n",
    "# Create a bar chart using Seaborn\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='rating', y='usefulCount', data=average_useful_count, palette='viridis')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Average Useful Count')\n",
    "plt.title('Average Useful Count for Each Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2df91",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- There is no real gain in usefulness other than for ratings 8 and above, which we saw earlier are also more prevalent than all other ratings except for 1.0, the lowest.\n",
    "\n",
    "Let's look at only the top 20% most useful reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average usefulCount for each rating\n",
    "average_useful_count = df[df.usefulCount > df.usefulCount.quantile(.80)].groupby('rating')['usefulCount'].mean().reset_index()\n",
    "\n",
    "# Create a bar chart using Seaborn\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x='rating', y='usefulCount', data=average_useful_count, palette='viridis')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Average Useful Count')\n",
    "plt.title('Average Useful Count for Each Rating — Top 20% of Useful Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70bc6d",
   "metadata": {},
   "source": [
    "There's a relative spike among ratings of 3.0. Let's look at some highly upvoted reviews with a rating of 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44469e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df[df.rating == 3].sort_values(by='usefulCount', ascending=False).index[:10]:\n",
    "    show_review(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb008c",
   "metadata": {},
   "source": [
    "Nothing jumps out about these reviews except that they are certainly negative and could all probably have been 1.0 ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e01ed7",
   "metadata": {},
   "source": [
    "## Term frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5cae4",
   "metadata": {},
   "source": [
    "Explain motivation for cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e3b39",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "We need stop lists for different purposes.\n",
    "\n",
    "For certain ngram analysis, we might like to consider keeping stop words like \"no\" and \"not\". This could be a \"light\" stop list.\n",
    "\n",
    "In other cases, we really want to trim it down, and stop as many words as possible. This could be a \"heavy\" stop list.\n",
    "\n",
    "Really need to clean up all stop list stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4578028",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [char for char in string.punctuation]\n",
    "punctuation_list.extend(['', '``', \"''\", '...'])\n",
    "\n",
    "# obtain the standard list of stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# start our own list of stopwords with these words\n",
    "stop_list_heavy = stopwords.words('english')\n",
    "# stop words to keep\n",
    "# 44-59 be/have/do verbs\n",
    "# 64-178 prepositions/subordinate conjunctions/modals\n",
    "stop_list_light = stop_list_heavy.copy()\n",
    "stop_list_light = stop_list_light[:44] + stop_list_light[60:64]\n",
    "# add punctuation characters\n",
    "for char in string.punctuation:\n",
    "    stop_list_light.append(char)\n",
    "    stop_list_heavy.append(char)\n",
    "# add misc other tokens\n",
    "stop_list_light.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])\n",
    "stop_list_heavy.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ec78a",
   "metadata": {},
   "source": [
    "### Create more usable versions of review feature\n",
    "\n",
    "- `review_lower` will only make everything lower-case but not otherwise remove stop words, punctuation, or alter (lemmatize) anything.\n",
    "- `review_simple` will be lemmatized and \"lightly\" stopped, keeping some be/have/do verbs as well as other modals, prepositions and negatives\n",
    "\n",
    "The \"heavy\" stop list (and other stop lists) can be used to further trim down `review_simple` if desired.\n",
    "\n",
    "NEED TO REVIEW THE REST OF THIS DOC TO EDIT INSTANCES OF STOP LISTS AND REVIEW_LOWER, ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a6d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lower'] = df.review.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cba4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: review (as a string)\n",
    "# output: review (as a string) — words are lemmatized, stopped, and separated by spaces (not commas as in a list)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def review_fix(review):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    stopped_tokens = [word for word in lemmatized_tokens if word not in stop_list_light]\n",
    "    review_fixed = ' '.join(stopped_tokens)\n",
    "\n",
    "    return review_fixed\n",
    "\n",
    "df['review_simple'] = df.review_lower.apply(review_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_lower.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8b9f1",
   "metadata": {},
   "source": [
    "## some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7157977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a list of documents and 1) tokenizes them, 2) lemmatizes them, and 3) removes stopwords\n",
    "# for example, to execute this function use make_tokens(df.review.tolist())\n",
    "# IN: a list of documents // OUT: a list of tokens\n",
    "\n",
    "def make_tokens(docs_list, stop_list=stop_list_light):\n",
    "    # join documents into a single string\n",
    "    docs_joined = ' '.join(docs_list)\n",
    "    # tokenize the single string into a list of tokens\n",
    "    tokens = nltk.word_tokenize(docs_joined)\n",
    "    # lemmatize the list of tokens\n",
    "    tokens_lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # stop the list of tokens\n",
    "    tokens_stopped = [word for word in tokens_lemmatized if word not in stop_list]\n",
    "    return tokens_stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ebfd8",
   "metadata": {},
   "source": [
    "### this section is dedicated to performing TFIDF in pairs (pos/neg) and displaying corresponding word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7baf1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stops= ['pill', 'month', 'control', 'birth', 'day', 'first', 'week', 'year', 'get', \\\n",
    "              'time', 'side', 'would', 'effect', 'like', 'never', '3', 'got', 'started', 'also', 'one', '2', \\\n",
    "              'feel', 'since', 'really', 'take', 'back', 'every', 'two', 'went', 'little', 'experience', \\\n",
    "              'even', 'put', 'recommend', 'taking', 'last', 'much', 'thing', 'almost', 'ever', 'going', 'far', \\\n",
    "              'getting', 'still', 'could', 'go', 'made', '4', 'took', 'lot', '5', 'felt', 'around', 'tried', \\\n",
    "              'three', 'using', 'say', 'second', 'mirena', 'skyla', 'iud', 'nuvaring', 'nuva', 'ring', 'use', \\\n",
    "              'thought', 'experienced', 'make', 'used', 'doe', 'implanon', 'nexplanon', 'alway', 'see', 'know', \\\n",
    "              'something', 'shot', 'always', 'depo', 'provera', 'away', 'injection', 'patch', 'xulane', 'third', \\\n",
    "              'ortho', 'evra', 'think', 'yaz', 'noticed', 'taken', 'implant', 'paragard', 'plan', 'b', 'period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83384b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_tokens = {}\n",
    "\n",
    "for method_ in df.method.unique():\n",
    "    stop_list_ = stop_list_heavy.copy()\n",
    "    stop_list_.extend(extra_stops)\n",
    "    tokens = make_tokens(df[df.method == method_].review_lower.tolist(), stop_list_)\n",
    "    method_tokens[method_] = {}\n",
    "    method_tokens[method_]['positive'] = make_tokens(df[(df.method == method_) \\\n",
    "                                                        & (df.rating > 5)].review_lower.to_list(), stop_list_)\n",
    "    method_tokens[method_]['negative'] = make_tokens(df[(df.method == method_) \\\n",
    "                                                        & (df.rating < 5)].review_lower.to_list(), stop_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_word_clouds(method_):\n",
    "\n",
    "    positive_text = (' ').join(method_tokens[method_]['positive'])\n",
    "    negative_text = (' ').join(method_tokens[method_]['negative'])\n",
    "\n",
    "    documents = [positive_text, negative_text]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    positive_tfidf_scores = tfidf_matrix[0]  # TF-IDF scores for positive document\n",
    "    negative_tfidf_scores = tfidf_matrix[1]  # TF-IDF scores for negative document\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create dictionaries with words and their corresponding TF-IDF scores for each document\n",
    "    positive_word_scores = dict(zip(feature_names, positive_tfidf_scores.toarray()[0]))\n",
    "    negative_word_scores = dict(zip(feature_names, negative_tfidf_scores.toarray()[0]))\n",
    "\n",
    "    # Generate word clouds for positive and negative documents\n",
    "    positive_wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        colormap='Greens',\n",
    "        collocations=True\n",
    "    ).generate_from_frequencies(positive_word_scores)\n",
    "    negative_wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        colormap='Reds',\n",
    "        collocations=True\n",
    "    ).generate_from_frequencies(negative_word_scores)\n",
    "\n",
    "    # show positive and negative wordclouds for apple side by side\n",
    "    fig, ax = plt.subplots(figsize=(32,24), ncols=2)\n",
    "    # plt.suptitle('Positive and Negative Word Clouds for Apple', fontsize=25)\n",
    "    ax[0].imshow(positive_wordcloud)\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(negative_wordcloud)\n",
    "    ax[1].axis('off')\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4440d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def method_word_clouds(method_):\n",
    "\n",
    "    positive_text = (' ').join(method_tokens[method_]['positive'])\n",
    "    negative_text = (' ').join(method_tokens[method_]['negative'])\n",
    "\n",
    "    documents = [positive_text, negative_text]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    positive_tfidf_scores = tfidf_matrix[0]  # TF-IDF scores for positive document\n",
    "    negative_tfidf_scores = tfidf_matrix[1]  # TF-IDF scores for negative document\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create dictionaries with words and their corresponding TF-IDF scores for each document\n",
    "    positive_word_scores = dict(zip(feature_names, positive_tfidf_scores.toarray()[0]))\n",
    "    negative_word_scores = dict(zip(feature_names, negative_tfidf_scores.toarray()[0]))\n",
    "\n",
    "    # Generate word clouds for positive and negative documents\n",
    "    positive_wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        colormap='Greens',\n",
    "        collocations=True\n",
    "    ).generate_from_frequencies(positive_word_scores)\n",
    "    negative_wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        colormap='Reds',\n",
    "        collocations=True\n",
    "    ).generate_from_frequencies(negative_word_scores)\n",
    "\n",
    "    # show positive and negative wordclouds for apple side by side\n",
    "    fig, ax = plt.subplots(figsize=(32,16), ncols=2)\n",
    "#     fig.subplots_adjust(top=0.85, bottom=0.15, hspace=0.3)\n",
    "    ax[0].imshow(positive_wordcloud)\n",
    "    ax[0].set_title('Positive Reviews', fontsize=65)\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(negative_wordcloud)\n",
    "    ax[1].set_title('Negative Reviews', fontsize=65)\n",
    "    ax[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80746ab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_word_clouds('IUD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e0693",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_word_clouds('emergency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e552b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_word_clouds('vaginal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905a324",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_word_clouds('injectable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab1210",
   "metadata": {},
   "source": [
    "————————————————————————————————————————————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594c947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "method_word_clouds('pill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de253f74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_word_clouds('patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1521f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_word_clouds('implantable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.review_lower.str.contains('breast')) & \\\n",
    "   (df.method == 'patch')\n",
    "  ].sort_values(by='rating', ascending=False).index[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7e927",
   "metadata": {},
   "source": [
    "————————————————————————————————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ef04d",
   "metadata": {},
   "source": [
    "### this section is work that may be retrieved and built upon to show individual word clouds\n",
    "\n",
    "If anything, the most important words stuff should factor in the extra_stops and be incorporated in the function that produces the pairs of word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that generates a word cloud of a given list of words\n",
    "# feed this function a LIST of tokens, i.e. the output of the make_tokens function\n",
    "# this function will comma-join the list for you\n",
    "def make_wordcloud(wordlist, colormap='Greens', title=None):\n",
    "    # instantiate wordcloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        colormap=colormap,\n",
    "        collocations = True\n",
    "    )\n",
    "    return wordcloud.generate(','.join(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that plots the word cloud\n",
    "# feed this function the output of the make_wordcloud function\n",
    "def plot_wordcloud(wordcloud):\n",
    "    # plot wordcloud\n",
    "    plt.figure(figsize = (12, 15)) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ec933",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, corpus, tokens, tokens_joined = {}, {}, {}, {}\n",
    "\n",
    "for method_ in df.method.unique():\n",
    "    reviews[method_] = df[df.method == method_].review_lower.tolist() # a list of all reviews of one method\n",
    "    corpus[method_] = ' '.join(reviews[method_]) # the above as a single string with spaces\n",
    "    tokens[method_] = nltk.word_tokenize(corpus[method_]) # the above as a list of words\n",
    "    tokens_joined[method_] = ' '.join(tokens[method_]) # the above as a single string with spaces\n",
    "\n",
    "tokens_joined_list = [tokens_joined[method_] for method_ in df.method.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a1b35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=stop_list_heavy)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tokens_joined_list)\n",
    "\n",
    "for i, doc in enumerate(tokens_joined_list):\n",
    "    print(f\"Important words in Document {i+1}:\")\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "    top_words = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:25]  # Adjust 5 to desired number of top words\n",
    "    for word_index, score in top_words:\n",
    "        print(f\"{feature_names[word_index]} (TF-IDF Score: {score:.2f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646d9be",
   "metadata": {},
   "source": [
    "————————————————————————————————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b544b",
   "metadata": {},
   "source": [
    "## What remains between here and the engineered features is of questionable use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d565e83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ind in df[df.method == 'implantable'].sort_values(by='usefulCount', ascending=False)[:10].index:\n",
    "    show_review(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.method.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d90310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('method').genericName.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('method').shortBrandName.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f836aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('method').usefulCount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bde88",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "\n",
    "uv = list(df.usefulCount.values)\n",
    "uv.sort(reverse=True)\n",
    "n = 0\n",
    "while sum(uv[:n]) < threshold / 100 * df.usefulCount.sum():\n",
    "    n += 1\n",
    "print(str(int(n * 100 / len(df)))+'%','of the reviews received', str(threshold)+'%', 'of all the upvotes.')\n",
    "print('This includes reviews with', uv[n], 'or more upvotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea125ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.usefulCount >= 41].rating.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 40\n",
    "\n",
    "uv = list(df.usefulCount.values)\n",
    "uv.sort(reverse=True)\n",
    "n = 0\n",
    "while sum(uv[:n]) < threshold / 100 * df.usefulCount.sum():\n",
    "    n += 1\n",
    "print(str(int(n * 100 / len(df)))+'%','of the reviews received', str(threshold)+'%', 'of all the upvotes.')\n",
    "print('This includes reviews with', uv[n], 'or more upvotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.usefulCount >= 20].rating.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 60\n",
    "\n",
    "uv = list(df.usefulCount.values)\n",
    "uv.sort(reverse=True)\n",
    "n = 0\n",
    "while sum(uv[:n]) < threshold / 100 * df.usefulCount.sum():\n",
    "    n += 1\n",
    "print(str(int(n * 100 / len(df)))+'%','of the reviews received', str(threshold)+'%', 'of all the upvotes.')\n",
    "print('This includes reviews with', uv[n], 'or more upvotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecfede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.usefulCount >= 11].rating.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80\n",
    "\n",
    "uv = list(df.usefulCount.values)\n",
    "uv.sort(reverse=True)\n",
    "n = 0\n",
    "while sum(uv[:n]) < threshold / 100 * df.usefulCount.sum():\n",
    "    n += 1\n",
    "print(str(int(n * 100 / len(df)))+'%','of the reviews received', str(threshold)+'%', 'of all the upvotes.')\n",
    "print('This includes reviews with', uv[n], 'or more upvotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.usefulCount >= 6].rating.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 95\n",
    "\n",
    "uv = list(df.usefulCount.values)\n",
    "uv.sort(reverse=True)\n",
    "n = 0\n",
    "while sum(uv[:n]) < threshold / 100 * df.usefulCount.sum():\n",
    "    n += 1\n",
    "print(str(int(n * 100 / len(df)))+'%','of the reviews received', str(threshold)+'%', 'of all the upvotes.')\n",
    "print('This includes reviews with', uv[n], 'or more upvotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.usefulCount >= 3].rating.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(0,len(df),100)\n",
    "Y = []\n",
    "for x in X:\n",
    "    Y.append(sum(uv[:x]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X,Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416205d8",
   "metadata": {},
   "source": [
    "This shows the cumulative sum of upvotes. For example, if we go 5% of the reviews in from the left on the x-axis, we would reach up to 33% of the total upvotes on the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932cffb",
   "metadata": {},
   "source": [
    "# feature engineering ideas to assist in modeling\n",
    "\n",
    "- ~~word count~~\n",
    "- ~~character count~~\n",
    "- ~~words in all caps~~\n",
    "- ~~average word length~~\n",
    "- whether words are in English (spelled correctly)\n",
    "- ~~whether it includes characters such as exclamation points, question marks~~\n",
    "- (especially repeatedly), and emoticons\n",
    "- ~~whether it mentions the brand~~\n",
    "- or generic name in the review\n",
    "- whether it uses slang versus technical language\n",
    "- whether it mentions touchy subjects such as abortion\n",
    "- ngrams\n",
    "- does it mention switching treatments, comparison to other methods\n",
    "- ~~\"but\" count~~\n",
    "- features that indicate whether certain side effects are discussed, e.g. weight gain, acne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb41244",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe395088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_useful(feature, low=0.15, high=0.85):\n",
    "    X = range(int(df[feature].quantile(low)), int(df[feature].quantile(high)))\n",
    "    Y = []\n",
    "    for x in X:\n",
    "        Y.append(df[df[feature] == x].usefulCount.mean())\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(X,Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14105bbc",
   "metadata": {},
   "source": [
    "## word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokens'] = df.review_lower.apply(\n",
    "    lambda x: [word for word in nltk.word_tokenize(x) if word not in punctuation_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df.word_tokens.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_useful('word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('word_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86edae",
   "metadata": {},
   "source": [
    "## character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['character_count'] = df.review_lower.apply(lambda x: len(re.findall(r'\\w', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_useful('character_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('character_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6803177",
   "metadata": {},
   "source": [
    "## sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_count'] = df.review_lower.apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_useful('sentence_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b292ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('sentence_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e9579",
   "metadata": {},
   "source": [
    "## Coleman-Liau Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLI_readability'] = df.apply(\n",
    "    lambda x: 5.88 * x.character_count / x.word_count - 29.6 * x.sentence_count / x.word_count - 15.8, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9364bc5",
   "metadata": {},
   "source": [
    "## Automated Readability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ARI_readability'] = df.apply(\n",
    "    lambda x: 4.71 * x.character_count / x.word_count + 0.5 * x.word_count / x.sentence_count - 21.43, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2e8f7",
   "metadata": {},
   "source": [
    "## buts / even-handed reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c120126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of words to search for in reviews\n",
    "nuanced_words = ['but', 'even though', 'conversely', 'nonetheless', 'notwithstanding', 'in spite of', 'admittedly', \\\n",
    "                 'however', 'although', 'yet', 'nevertheless', 'on the other hand', 'despite', 'even so', 'while']\n",
    "\n",
    "# Function to count occurrences of nuanced words in a review\n",
    "def count_nuanced_words(text):\n",
    "    text_lower = text.lower()\n",
    "    count = sum(text_lower.count(word) for word in nuanced_words)\n",
    "    return count\n",
    "\n",
    "# Assuming df is your DataFrame and 'review_text' is the column containing the reviews\n",
    "df['nuanced_words_count'] = df['review_lower'].apply(count_nuanced_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame containing 'but_count' and 'rating' columns\n",
    "\n",
    "# Grouping by 'rating' and calculating the mean of 'but_count' for each rating\n",
    "but_count_by_rating = df.groupby('rating')['nuanced_words_count'].mean()\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "but_count_by_rating.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Count of Nuanced Words for Each Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Average Nuanced Words Count')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels if needed\n",
    "plt.grid(axis='y')  # Add gridlines for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc20b9a",
   "metadata": {},
   "source": [
    "## all caps words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_caps_fix(review):\n",
    "    review_tokenized = [word for word in nltk.word_tokenize(review) if word not in punctuation_list]\n",
    "    all_caps_words = [word for word in review_tokenized if len(word) > 2 and word.isupper()]\n",
    "    return len(all_caps_words)\n",
    "\n",
    "df['all_caps_word_count'] = df.review.apply(lambda x: all_caps_fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.all_caps_word_count.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_useful('all_caps_word_count', high=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('all_caps_word_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8349a",
   "metadata": {},
   "source": [
    "## no caps usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b685ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_caps'] = df.apply(lambda x: 1 if x.review == x.review_lower else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.no_caps.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61caceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.no_caps == 1].usefulCount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.no_caps == 0].usefulCount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e066ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('no_caps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694081c",
   "metadata": {},
   "source": [
    "## exclaim count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exclaim_count'] = df.review.apply(lambda x: x.count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9172f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exclaim_count.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_useful('exclaim_count', high=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('exclaim_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc167b",
   "metadata": {},
   "source": [
    "## question count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question_count'] = df.review.apply(lambda x: x.count('?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82841f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.question_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_useful('question_count', low=0.01, high=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('question_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99de4f3",
   "metadata": {},
   "source": [
    "## age disclosure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_disclosure_feature(review):\n",
    "    if re.search('[1-4][0-9] +y(ea)?rs? +(old|of age)', review) or \\\n",
    "    re.search('(pregnant( +at)?|age( +of)?|being) +[1-4][0-9]', review) or \\\n",
    "    re.search('(teen|ty|one|two|three|four|five|six|seven|eight|nine) +y(ea)?rs? +(old|of +age)', review) or \\\n",
    "    re.search('i +(am +|was +|will +be +|would have been +)(not even +)?(now +)?(only +)?(just +)?(maybe +)?[1-4][0-9](\\s|\\.|\\!|\\?)', review):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df['age_disclosure'] = df.review_lower.apply(lambda x: 1 if age_disclosure_feature(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f97ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age_disclosure.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab753b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.age_disclosure == 1].usefulCount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.age_disclosure == 0].usefulCount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca870096",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features.append('age_disclosure')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ffd8a",
   "metadata": {},
   "source": [
    "## inclusion of brand name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_brand_dict = {}\n",
    "\n",
    "def brand_by_method_fix(review, method_brand_list):\n",
    "    for brand in method_brand_list:\n",
    "        if brand in review:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "for method_ in df.method.unique():\n",
    "    method_brand_list = list(df[(df.method == method_) & \\\n",
    "                                         (df.shortBrandName.notna())].shortBrandName.unique())\n",
    "    method_brand_string = ' '.join([word.lower() for word in method_brand_list])\n",
    "    method_brand_list = list(set(method_brand_string.split(' ')))\n",
    "    df[method_] = df.review_lower.apply(lambda x: brand_by_method_fix(x, method_brand_list))\n",
    "    engineered_features.append(method_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c29356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT re-run this cell out of sequence\n",
    "# to use the dataframe as it was at this stage, un-comment, run, and re-comment the cell that follows after it.\n",
    "df_bookmark_5 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193651e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_bookmark_5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5a8ef",
   "metadata": {},
   "source": [
    "# save and reload preprocessed set\n",
    "\n",
    "At this stage we will save and reload the preprocessed set in order to avoid taking the time to repeat earlier work everytime we open the notebook.\n",
    "\n",
    "The saved version has restored or deleted all records with missing condition labels.\n",
    "\n",
    "We have established pairs in the list `twins` but we have NOT yet deleted either member of any pair or dealt with the confusion between brand and generic drug names.\n",
    "\n",
    "The size of the dateframe is nearly the same as its original version, roughly 215,000 records.\n",
    "\n",
    "In the future, store the functions with this?\n",
    "\n",
    "Store the engineered features list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('../data/preprocessed.csv')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3620e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store engineered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51faaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r engineered_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a3d16",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Predicting birth control method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf307e",
   "metadata": {},
   "source": [
    "## The target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a66821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.method.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185278f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(df[['review'] + engineered_features], df['target'], test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376feb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this value to compare to future model crossval scores\n",
    "plurality_cv = round(y_train.value_counts(normalize=True)[1],4)\n",
    "# show the sentiment breakdown\n",
    "round(y_train.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a94de8",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80a9d7",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d942445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset variables\n",
    "\n",
    "engineered_features = []\n",
    "text_preprocessor = None\n",
    "numerical_preprocessor = None\n",
    "preprocessor = None\n",
    "pipeline = None\n",
    "accuracy = None\n",
    "feature_names = None\n",
    "coefficients = None\n",
    "decision_function_values = None\n",
    "importance_df = None\n",
    "feature_importance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409733f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "stop_words = stop_list\n",
    "ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    ngram_range=ngram_range\n",
    ")\n",
    "\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_preprocessor, 'review'),\n",
    "        ('numerical', numerical_preprocessor, engineered_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # display the training and test accuracy scores\n",
    "    print(f\"Training Score: {round(pipeline.score(X_train, y_train),4)} \\\n",
    "    \\nTest Score:     {round(pipeline.score(X_test, y_test),4)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "#     y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for log loss\n",
    "    \n",
    "    # calculate different evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#     logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    # display different evaluation metrics\n",
    "    print(f\"Accuracy Score: {round(accuracy, 4)}\")\n",
    "#     print(f\"Recall Score: {round(recall, 4)}\")\n",
    "#     print(f\"Precision Score: {round(precision, 4)}\")\n",
    "#     print(f\"F1 Score: {round(f1, 4)}\")\n",
    "#     print(f\"ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "#     print(f\"Log Loss: {round(logloss, 4)}\")\n",
    "    \n",
    "    # plot the normalized confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Modify the figsize as per your preference\n",
    "    disp.plot(cmap='Greens', ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "#     plot_confusion_matrix(estimator=pipeline, X=X_test, y_true=y_test, cmap='Greens', \n",
    "#                           normalize='true', \n",
    "#                           display_labels=['Non-positive', 'Positive'])\n",
    "\n",
    "\n",
    "\n",
    "# >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "# >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "# ...                               display_labels=clf.classes_)\n",
    "# >>> disp.plot()\n",
    "# <...>\n",
    "# >>> plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce470e17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for model in [LogisticRegression(max_iter=1000), DecisionTreeClassifier(random_state=SEED), \\\n",
    "              BaggingClassifier(), RandomForestClassifier(random_state=SEED), \\\n",
    "              AdaBoostClassifier(random_state=SEED), GradientBoostingClassifier(random_state=SEED), \\\n",
    "              XGBClassifier(random_state=SEED)]:\n",
    "    run_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b168e1f",
   "metadata": {},
   "source": [
    "dry run with of all the models:\n",
    "\n",
    "XGB was the best, at 93.76% accuracy (train was 99.83%)\n",
    "\n",
    "Logistic, Bagging, and Gradient were all at or above 90% accuracy\n",
    "\n",
    "AdaBoost was the least overfit, train/test was 87/86\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced703c",
   "metadata": {},
   "source": [
    "## Decision tree tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b925a54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__criterion': ['gini', 'entropy'], \n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_leaf': [1, 2, 3]\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__criterion': 'gini', 'model__max_depth': None, 'model__min_samples_leaf': 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc823cc3",
   "metadata": {},
   "source": [
    "## Random forest tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ccf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__criterion': ['gini', 'entropy'], \n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_leaf': [1, 2, 3]\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__criterion': 'gini', 'model__max_depth': None, 'model__min_samples_leaf': 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637cf86",
   "metadata": {},
   "source": [
    "## Adaboost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [50, 100, 200],  # Number of estimators (weak learners)\n",
    "            'model__learning_rate': [0.1, 0.5, 1.0],  # Learning rate for the updates\n",
    "            'model__estimator__max_depth': [1, 2, 3]  # Max depth of the weak learners (Decision Trees)\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__estimator__max_depth': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 200}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1084683",
   "metadata": {},
   "source": [
    "## Logistic regression tuning\n",
    "\n",
    "Beware this one took over 6 minutes to tune on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f8061",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {\n",
    "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "        'model__penalty': ['l1', 'l2'],  # Regularization penalty ('l1' for Lasso, 'l2' for Ridge)\n",
    "        'model__solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__C': 100, 'model__penalty': 'l1', 'model__solver': 'liblinear'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9af477",
   "metadata": {},
   "source": [
    "## Bagged trees tuning\n",
    "\n",
    "Beware this took over 12 minutes to tune on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8587b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', BaggingClassifier(estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "    \n",
    "    param_grid = {\n",
    "        'model__n_estimators': [10, 50, 100],  # Number of base estimators (decision trees in this case)\n",
    "        'model__max_samples': [0.5, 0.7, 1.0],  # Sample size for each base estimator\n",
    "        'model__max_features': [0.5, 0.7, 1.0],  # Number of features to consider for each base estimator\n",
    "        'model__estimator__max_depth': [None, 5, 10]  # Max depth of the decision trees\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__estimator__max_depth': None, 'model__max_features': 0.5, \\\n",
    "    'model__max_samples': 1.0, 'model__n_estimators': 100} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715c8d3",
   "metadata": {},
   "source": [
    "## Gradient boost tuning\n",
    "\n",
    "This simply took too long and should not be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96700c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [50, 100, 200],  # Number of estimators (weak learners)\n",
    "                  'model__learning_rate': [0.1, 0.01, 0.001],  # Step size shrinkage used to prevent overfitting\n",
    "                  'model__max_depth': [3, 5, 7],  # Maximum depth of the individual trees\n",
    "                  'model__subsample': [0.8, 1.0]  # Subsample ratio of the training instance\n",
    "                 }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2672fb",
   "metadata": {},
   "source": [
    "## XGB tuning\n",
    "\n",
    "Based on running 30 seconds with single parameters, this could take nearly an hour to run on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12eecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [100, 200, 300],  # Number of boosting rounds (trees)\n",
    "                  'model__learning_rate': [0.1, 0.01, 0.001],  # Step size shrinkage used to prevent overfitting\n",
    "                  'model__max_depth': [3, 5, 7],  # Maximum depth of each tree\n",
    "                  'model__subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "                  'model__colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "                 }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582d868",
   "metadata": {},
   "source": [
    "## Predicting rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40204191",
   "metadata": {},
   "source": [
    "## The target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_threshold = [2, 10]\n",
    "\n",
    "print(len(df[df.rating <= rating_threshold[0]]), len(df[df.rating >= rating_threshold[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa807dc",
   "metadata": {},
   "source": [
    "The above would be the easiest split. It uses fewer values and reduces class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_threshold = [7, 8]\n",
    "\n",
    "print(len(df[df.rating <= rating_threshold[0]]), len(df[df.rating >= rating_threshold[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f2a2e",
   "metadata": {},
   "source": [
    "The above would be the best way to reduce class imbalance while using all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_threshold = [4, 6]\n",
    "\n",
    "print(len(df[df.rating <= rating_threshold[0]]), len(df[df.rating >= rating_threshold[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316bebe7",
   "metadata": {},
   "source": [
    "The above would use nearly all the data and give a better approximation of positive v. negative. It introduces a more significant class imbalance, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[\n",
    "    (df.rating > rating_threshold[0]) & \\\n",
    "    (df.rating < rating_threshold[1])\n",
    "].index, inplace=True)\n",
    "\n",
    "df['target'] = df.rating.apply(lambda x: 1 if x >= rating_threshold[1] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71066cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(df[['review'] + engineered_features], df['target'], test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this value to compare to future model crossval scores\n",
    "plurality_cv = round(y_train.value_counts(normalize=True)[1],4)\n",
    "# show the sentiment breakdown\n",
    "round(y_train.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e5f62b",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f27f88",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset variables\n",
    "\n",
    "engineered_features = []\n",
    "text_preprocessor = None\n",
    "numerical_preprocessor = None\n",
    "preprocessor = None\n",
    "pipeline = None\n",
    "accuracy = None\n",
    "feature_names = None\n",
    "coefficients = None\n",
    "decision_function_values = None\n",
    "importance_df = None\n",
    "feature_importance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fab603",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "stop_words = stop_list\n",
    "ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    ngram_range=ngram_range\n",
    ")\n",
    "\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_preprocessor, 'review'),\n",
    "        ('numerical', numerical_preprocessor, engineered_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b339b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # display the training and test accuracy scores\n",
    "    print(f\"Training Score: {round(pipeline.score(X_train, y_train),4)} \\\n",
    "    \\nTest Score:     {round(pipeline.score(X_test, y_test),4)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "#     y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for log loss\n",
    "    \n",
    "    # calculate different evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#     logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    # display different evaluation metrics\n",
    "    print(f\"Accuracy Score: {round(accuracy, 4)}\")\n",
    "#     print(f\"Recall Score: {round(recall, 4)}\")\n",
    "#     print(f\"Precision Score: {round(precision, 4)}\")\n",
    "#     print(f\"F1 Score: {round(f1, 4)}\")\n",
    "#     print(f\"ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "#     print(f\"Log Loss: {round(logloss, 4)}\")\n",
    "    \n",
    "    # plot the normalized confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Modify the figsize as per your preference\n",
    "    disp.plot(cmap='Greens', ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "#     plot_confusion_matrix(estimator=pipeline, X=X_test, y_true=y_test, cmap='Greens', \n",
    "#                           normalize='true', \n",
    "#                           display_labels=['Non-positive', 'Positive'])\n",
    "\n",
    "\n",
    "\n",
    "# >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "# >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "# ...                               display_labels=clf.classes_)\n",
    "# >>> disp.plot()\n",
    "# <...>\n",
    "# >>> plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8df69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for model in [LogisticRegression(max_iter=1000), DecisionTreeClassifier(random_state=SEED), \\\n",
    "              BaggingClassifier(), RandomForestClassifier(random_state=SEED), \\\n",
    "              AdaBoostClassifier(random_state=SEED), GradientBoostingClassifier(random_state=SEED), \\\n",
    "              XGBClassifier(random_state=SEED)]:\n",
    "    run_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ba27a",
   "metadata": {},
   "source": [
    "dry run with of all the models:\n",
    "\n",
    "XGB was the best, at 93.76% accuracy (train was 99.83%)\n",
    "\n",
    "Logistic, Bagging, and Gradient were all at or above 90% accuracy\n",
    "\n",
    "AdaBoost was the least overfit, train/test was 87/86\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148690e",
   "metadata": {},
   "source": [
    "## Decision tree tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2591c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__criterion': ['gini', 'entropy'], \n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_leaf': [1, 2, 3]\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__criterion': 'gini', 'model__max_depth': None, 'model__min_samples_leaf': 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adf5f4",
   "metadata": {},
   "source": [
    "## Random forest tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__criterion': ['gini', 'entropy'], \n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_leaf': [1, 2, 3]\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__criterion': 'gini', 'model__max_depth': None, 'model__min_samples_leaf': 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff69f53",
   "metadata": {},
   "source": [
    "## Adaboost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76696675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [50, 100, 200],  # Number of estimators (weak learners)\n",
    "            'model__learning_rate': [0.1, 0.5, 1.0],  # Learning rate for the updates\n",
    "            'model__estimator__max_depth': [1, 2, 3]  # Max depth of the weak learners (Decision Trees)\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__estimator__max_depth': 3, 'model__learning_rate': 0.1, 'model__n_estimators': 200}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253be6c",
   "metadata": {},
   "source": [
    "## Logistic regression tuning\n",
    "\n",
    "Beware this one took over 6 minutes to tune on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0f1fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {\n",
    "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "        'model__penalty': ['l1', 'l2'],  # Regularization penalty ('l1' for Lasso, 'l2' for Ridge)\n",
    "        'model__solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__C': 100, 'model__penalty': 'l1', 'model__solver': 'liblinear'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df66a72",
   "metadata": {},
   "source": [
    "## Bagged trees tuning\n",
    "\n",
    "Beware this took over 12 minutes to tune on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6430075",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', BaggingClassifier(estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "    \n",
    "    param_grid = {\n",
    "        'model__n_estimators': [10, 50, 100],  # Number of base estimators (decision trees in this case)\n",
    "        'model__max_samples': [0.5, 0.7, 1.0],  # Sample size for each base estimator\n",
    "        'model__max_features': [0.5, 0.7, 1.0],  # Number of features to consider for each base estimator\n",
    "        'model__estimator__max_depth': [None, 5, 10]  # Max depth of the decision trees\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__estimator__max_depth': None, 'model__max_features': 0.5, \\\n",
    "    'model__max_samples': 1.0, 'model__n_estimators': 100} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a25bd2",
   "metadata": {},
   "source": [
    "## Gradient boost tuning\n",
    "\n",
    "This simply took too long and should not be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03db07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [50, 100, 200],  # Number of estimators (weak learners)\n",
    "                  'model__learning_rate': [0.1, 0.01, 0.001],  # Step size shrinkage used to prevent overfitting\n",
    "                  'model__max_depth': [3, 5, 7],  # Maximum depth of the individual trees\n",
    "                  'model__subsample': [0.8, 1.0]  # Subsample ratio of the training instance\n",
    "                 }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb284d3",
   "metadata": {},
   "source": [
    "## XGB tuning\n",
    "\n",
    "Based on running 30 seconds with single parameters, this could take nearly an hour to run on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [100, 200, 300],  # Number of boosting rounds (trees)\n",
    "                  'model__learning_rate': [0.1, 0.01, 0.001],  # Step size shrinkage used to prevent overfitting\n",
    "                  'model__max_depth': [3, 5, 7],  # Maximum depth of each tree\n",
    "                  'model__subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "                  'model__colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "                 }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf09a2",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8de29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 25\n",
    "title = '_____ Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41421613",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = preprocessor.named_transformers_['text'].get_feature_names_out().tolist() + engineered_features\n",
    "\n",
    "# --------------------------------------------------------\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    coefficients = model.feature_importances_\n",
    "elif hasattr(model.coef_, 'toarray'):\n",
    "    coefficients = model.coef_.toarray().flatten()\n",
    "else:\n",
    "    coefficients = model.coef_.flatten()\n",
    "# --------------------------------------------------------\n",
    "\n",
    "importance_df = pd.DataFrame(feature_names, columns=['Word'])\n",
    "importance_df['Importance'] = np.e**(abs(coefficients))\n",
    "importance_df['Coefficient'] = coefficients\n",
    "\n",
    "feature_importance = importance_df.sort_values(\n",
    "    by = [\"Importance\"], ascending=False\n",
    ").head(n_features)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10), ncols=2)\n",
    "ax[0].set_title(f'Coefficients for {title}')\n",
    "ax[0].set_ylabel('Word')\n",
    "ax[0].set_xlabel('Coefficient')\n",
    "sns.barplot(x='Coefficient', y='Word', data=feature_importance, \n",
    "            palette='coolwarm', ax=ax[0])\n",
    "#plotting feature importances\n",
    "ax[1].set_title(f'Feature Importances for {title}')\n",
    "ax[1].set_ylabel('Word')\n",
    "ax[1].set_xlabel('Importance')\n",
    "sns.barplot(x='Importance', y='Word', data=feature_importance, \n",
    "            palette='coolwarm', ax=ax[1])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db6af6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace964b2",
   "metadata": {},
   "source": [
    "# Recommendations\n",
    "\n",
    "Don't know where else to put this right now. At least one review, 27228, is glowingly positive but gave a 1.0 rating, clearly misunderstanding the rating system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8687d2",
   "metadata": {},
   "source": [
    "## Use a different rating system\n",
    "\n",
    "No more than 5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92560db3",
   "metadata": {},
   "source": [
    "# Further Inquiry\n",
    "\n",
    "- Recommendation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef61da",
   "metadata": {},
   "source": [
    "## Light version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3626f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = np.random.choice(df.index, 250, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index.isin(random_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf95964",
   "metadata": {},
   "source": [
    "———————————————————————————————————————————————————————————————————"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
