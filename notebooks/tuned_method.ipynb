{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5748a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import html\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 1979\n",
    "\n",
    "do_grids = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467745a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r engineered_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cfb75",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066cdff",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade08cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [char for char in string.punctuation]\n",
    "punctuation_list.extend(['', '``', \"''\", '...'])\n",
    "\n",
    "# obtain the standard list of stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# start our own list of stopwords with these words\n",
    "stop_list_heavy = stopwords.words('english')\n",
    "# stop words to keep\n",
    "# 44-59 be/have/do verbs\n",
    "# 64-178 prepositions/subordinate conjunctions/modals\n",
    "stop_list_light = stop_list_heavy.copy()\n",
    "stop_list_light = stop_list_light[:44] + stop_list_light[60:64]\n",
    "# add punctuation characters\n",
    "for char in string.punctuation:\n",
    "    stop_list_light.append(char)\n",
    "    stop_list_heavy.append(char)\n",
    "# add misc other tokens\n",
    "stop_list_light.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])\n",
    "stop_list_heavy.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ba858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409fe223",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(df[['review'] + engineered_features], df['target'], test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b23633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this value to compare to future model crossval scores\n",
    "plurality_cv = round(y_train.value_counts(normalize=True)[1],4)\n",
    "# show the sentiment breakdown\n",
    "round(y_train.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c804d",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc997e",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "stop_words = stop_list_light\n",
    "ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    ngram_range=ngram_range\n",
    ")\n",
    "\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_preprocessor, 'review'),\n",
    "        ('numerical', numerical_preprocessor, engineered_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_1(model):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # display the training and test accuracy scores\n",
    "    print(f\"Training Score: {round(pipeline.score(X_train, y_train),4)} \\\n",
    "    \\nTest Score:     {round(pipeline.score(X_test, y_test),4)}\")\n",
    "    \n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # calculate different evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "#     roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # display different evaluation metrics\n",
    "    print(f\"\\nAccuracy Score: {round(accuracy, 4)}\")\n",
    "    print(f\"F1 Macro Score: {round(f1_macro, 4)}\")\n",
    "#     print(f\"ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "    \n",
    "    # plot the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap='Greens', ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_1(model, param_grid):\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])\n",
    "    param_grid = param_grid\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='f1_macro')\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    return gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b4453",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3a708",
   "metadata": {},
   "source": [
    "## Tuning the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if do_grids == True:\n",
    "    best_params_ = tune_model_1(LogisticRegression(random_state=SEED, max_iter=1000), param_grid={\n",
    "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "        'model__penalty': ['l1', 'l2'],  # Regularization penalty ('l1' for Lasso, 'l2' for Ridge)\n",
    "        'model__solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem\n",
    "    })\n",
    "else:\n",
    "    best_params_ = \"{'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\"\n",
    "print(best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c728de",
   "metadata": {},
   "source": [
    "## Logistic regression — tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608831a",
   "metadata": {},
   "source": [
    "## Bagged trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64435d2",
   "metadata": {},
   "source": [
    "## Tuning the bagged trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba285a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if do_grids == True:\n",
    "    best_params_ = tune_model_1(BaggingClassifier(random_state=SEED, estimator=DecisionTreeClassifier()), param_grid={\n",
    "        'model__n_estimators': [10, 50, 100],  # Number of base estimators (decision trees in this case)\n",
    "        'model__max_samples': [0.5, 0.7, 1.0],  # Sample size for each base estimator\n",
    "        'model__max_features': [0.5, 0.7, 1.0],  # Number of features to consider for each base estimator\n",
    "        'model__estimator__max_depth': [None, 5, 10]  # Max depth of the decision trees\n",
    "    })\n",
    "else:\n",
    "    best_params_ = \"{'model__estimator__max_depth': None, 'model__max_features': 0.7, \\\n",
    "    'model__max_samples': 1.0, 'model__n_estimators': 100}\"\n",
    "print(best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73e48e",
   "metadata": {},
   "source": [
    "## Bagged trees — tuned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
