{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbff6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import html\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 1979\n",
    "\n",
    "do_grids = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ab6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db401e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r engineered_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57950754",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd80a25",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae39484",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [char for char in string.punctuation]\n",
    "punctuation_list.extend(['', '``', \"''\", '...'])\n",
    "\n",
    "# obtain the standard list of stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# start our own list of stopwords with these words\n",
    "stop_list_heavy = stopwords.words('english')\n",
    "# stop words to keep\n",
    "# 44-59 be/have/do verbs\n",
    "# 64-178 prepositions/subordinate conjunctions/modals\n",
    "stop_list_light = stop_list_heavy.copy()\n",
    "stop_list_light = stop_list_light[:44] + stop_list_light[60:64]\n",
    "# add punctuation characters\n",
    "for char in string.punctuation:\n",
    "    stop_list_light.append(char)\n",
    "    stop_list_heavy.append(char)\n",
    "# add misc other tokens\n",
    "stop_list_light.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])\n",
    "stop_list_heavy.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba374f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7249 12948\n"
     ]
    }
   ],
   "source": [
    "rating_threshold = [4, 6]\n",
    "\n",
    "print(len(df[df.rating <= rating_threshold[0]]), len(df[df.rating >= rating_threshold[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c623f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[\n",
    "    (df.rating > rating_threshold[0]) & \\\n",
    "    (df.rating < rating_threshold[1])\n",
    "].index, inplace=True)\n",
    "\n",
    "df['target'] = df.rating.apply(lambda x: 1 if x >= rating_threshold[1] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e54d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(df[['review'] + engineered_features], df['target'], test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15383027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    0.6416\n",
       "0    0.3584\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save this value to compare to future model crossval scores\n",
    "plurality_cv = round(y_train.value_counts(normalize=True)[1],4)\n",
    "# show the sentiment breakdown\n",
    "round(y_train.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fdf97",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ff6bd",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bedf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "stop_words = stop_list_light\n",
    "ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd614e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    ngram_range=ngram_range\n",
    ")\n",
    "\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_preprocessor, 'review'),\n",
    "        ('numerical', numerical_preprocessor, engineered_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28a902",
   "metadata": {},
   "source": [
    "## Decision tree tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c9a2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__criterion': ['gini', 'entropy'], \n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_leaf': [1, 2, 3]\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__criterion': 'gini', 'model__max_depth': 20, 'model__min_samples_leaf': 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25a1c5",
   "metadata": {},
   "source": [
    "## Random forest tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e29c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__criterion': ['gini', 'entropy'], \n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_leaf': [1, 2, 3]\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__criterion': 'gini', 'model__max_depth': None, 'model__min_samples_leaf': 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f1ab4",
   "metadata": {},
   "source": [
    "## Adaboost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', AdaBoostClassifier(estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {'model__n_estimators': [50, 100, 200],  # Number of estimators (weak learners)\n",
    "            'model__learning_rate': [0.1, 0.5, 1.0],  # Learning rate for the updates\n",
    "            'model__estimator__max_depth': [1, 2, 3]  # Max depth of the weak learners (Decision Trees)\n",
    "           }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__estimator__max_depth': 1, 'model__learning_rate': 1.0, 'model__n_estimators': 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2299e",
   "metadata": {},
   "source": [
    "## Logistic regression tuning\n",
    "\n",
    "Beware this one took over 6 minutes to tune on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41e56b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "\n",
    "    param_grid = {\n",
    "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "        'model__penalty': ['l1', 'l2'],  # Regularization penalty ('l1' for Lasso, 'l2' for Ridge)\n",
    "        'model__solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4862f3",
   "metadata": {},
   "source": [
    "## Bagged trees tuning\n",
    "\n",
    "Beware this took over 12 minutes to tune on the light set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', BaggingClassifier(estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "if do_grids == True:\n",
    "    \n",
    "    param_grid = {\n",
    "        'model__n_estimators': [10, 50, 100],  # Number of base estimators (decision trees in this case)\n",
    "        'model__max_samples': [0.5, 0.7, 1.0],  # Sample size for each base estimator\n",
    "        'model__max_features': [0.5, 0.7, 1.0],  # Number of features to consider for each base estimator\n",
    "        'model__estimator__max_depth': [None, 5, 10]  # Max depth of the decision trees\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator=pipeline, param_grid = param_grid, scoring='accuracy')\n",
    "\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    print(gridsearch.best_params_,'\\n')\n",
    "else:\n",
    "    print(\"{'model__estimator__max_depth': None, 'model__max_features': 0.7, \\\n",
    "    'model__max_samples': 1.0, 'model__n_estimators': 100} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eba09f",
   "metadata": {},
   "source": [
    "## Gradient boost tuning\n",
    "\n",
    "This simply took too long and should not be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f270480",
   "metadata": {},
   "source": [
    "## XGB tuning\n",
    "\n",
    "Based on running 30 seconds with single parameters, this could take nearly an hour to run on the light set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
