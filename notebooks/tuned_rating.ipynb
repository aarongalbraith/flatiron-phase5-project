{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import html\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 1979\n",
    "\n",
    "do_grids = True\n",
    "\n",
    "light_frame = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/preprocessed.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r engineered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f49285",
   "metadata": {},
   "outputs": [],
   "source": [
    "if light_frame == True:\n",
    "    random_df = np.random.choice(df.index, 1000, replace=False)\n",
    "    df = df[df.index.isin(random_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbed29",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a1c7",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [char for char in string.punctuation]\n",
    "punctuation_list.extend(['', '``', \"''\", '...'])\n",
    "\n",
    "# obtain the standard list of stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# start our own list of stopwords with these words\n",
    "stop_list_heavy = stopwords.words('english')\n",
    "# stop words to keep\n",
    "# 44-59 be/have/do verbs\n",
    "# 64-178 prepositions/subordinate conjunctions/modals\n",
    "stop_list_light = stop_list_heavy.copy()\n",
    "stop_list_light = stop_list_light[:44] + stop_list_light[60:64]\n",
    "# add punctuation characters\n",
    "for char in string.punctuation:\n",
    "    stop_list_light.append(char)\n",
    "    stop_list_heavy.append(char)\n",
    "# add misc other tokens\n",
    "stop_list_light.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])\n",
    "stop_list_heavy.extend(['', 'll', 're', 've', 'ha', 'wa', '``', \"''\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_threshold = [4, 6]\n",
    "\n",
    "neg_count = len(df[df.rating <= rating_threshold[0]])\n",
    "pos_count = len(df[df.rating >= rating_threshold[1]])\n",
    "neg_freq = neg_count / (neg_count + pos_count)\n",
    "pos_freq = pos_count / (neg_count + pos_count)\n",
    "\n",
    "print(neg_count, pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[\n",
    "    (df.rating > rating_threshold[0]) & \\\n",
    "    (df.rating < rating_threshold[1])\n",
    "].index, inplace=True)\n",
    "\n",
    "df['target'] = df.rating.apply(lambda x: 1 if x <= rating_threshold[0] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87929b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(df[['review'] + engineered_features], df['target'], test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9faa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this value to compare to future model crossval scores\n",
    "plurality_cv = round(y_train.value_counts(normalize=True)[1],4)\n",
    "# show the sentiment breakdown\n",
    "round(y_train.value_counts(normalize=True),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfebf48",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19653a2d",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec272e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "stop_words = stop_list_light\n",
    "ngram_range = (1,3)\n",
    "class_weight = {\n",
    "    0: neg_freq,\n",
    "    1: pos_freq/neg_freq\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    ngram_range=ngram_range\n",
    ")\n",
    "\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_preprocessor, 'review'),\n",
    "        ('numerical', numerical_preprocessor, engineered_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da84e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_2(model):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # display the training and test accuracy scores\n",
    "    print(f\"Training Score: {round(pipeline.score(X_train, y_train),4)} \\\n",
    "    \\nTest Score:     {round(pipeline.score(X_test, y_test),4)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate predictions for the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for log loss\n",
    "    \n",
    "    # calculate different evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#     logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    # display different evaluation metrics\n",
    "    print(f\"\\nAccuracy Score: {round(accuracy, 4)}\")\n",
    "    print(f\"Recall Score: {round(recall, 4)}\")\n",
    "#     print(f\"Precision Score: {round(precision, 4)}\")\n",
    "    print(f\"F1 Score: {round(f1, 4)}\")\n",
    "    print(f\"ROC-AUC Score: {round(roc_auc, 4)}\")\n",
    "#     print(f\"Log Loss: {round(logloss, 4)}\")\n",
    "    \n",
    "    # plot the normalized confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Modify the figsize as per your preference\n",
    "    disp.plot(cmap='Greens', ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_2(model, param_grid):\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])\n",
    "    param_grid = param_grid\n",
    "    gridsearch = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid = param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "    )\n",
    "    gridsearch.fit(X_train,  y_train)\n",
    "    gridsearch.best_params_\n",
    "    return gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefe61c",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e299e",
   "metadata": {},
   "source": [
    "## Tuning the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5520e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if do_grids == True:\n",
    "    best_params_ = tune_model_2(LogisticRegression(random_state=SEED, class_weight=class_weight, max_iter=1000), param_grid={\n",
    "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "        'model__penalty': ['l1', 'l2'],  # Regularization penalty ('l1' for Lasso, 'l2' for Ridge)\n",
    "        'model__solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem\n",
    "    })\n",
    "else:\n",
    "    best_params_ = \"{'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\"\n",
    "print(best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ab769",
   "metadata": {},
   "source": [
    "## Logistic regression â€” tuned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
